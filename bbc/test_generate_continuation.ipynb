{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt 1: Once upon a time\n",
      "Generated Text: Once upon a time\n",
      "\n",
      "The sun shines in the sky\n",
      "\n",
      "And the moon shines in the sky\n",
      "\n",
      "And the stars shine in the sky\n",
      "\n",
      "And the stars shine in the sky\n",
      "\n",
      "And the stars shine in the\n",
      "\n",
      "Prompt 2: In a galaxy far, far away\n",
      "Generated Text: In a galaxy far, far away, the galaxy is a vast, vast, vast, vast, vast, vast, vast, vast, vast, vast, vast, vast, vast, vast, vast, vast, vast, vast, vast,\n",
      "\n",
      "Prompt 3: The quick brown fox\n",
      "Generated Text: The quick brown foxThe quick brown foxThe quick brown foxThe quick brown foxThe quick brown foxThe quick brown foxThe quick brown foxThe quick brown foxThe quick brown foxThe quick brown foxThe quick brown foxThe quick brown\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_name = 'gpt2'\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "model.eval()\n",
    "\n",
    "# Define prompts\n",
    "prompts = [\n",
    "    \"Once upon a time\",\n",
    "    \"In a galaxy far, far away\",\n",
    "    \"The quick brown fox\"\n",
    "]\n",
    "\n",
    "# Tokenize with padding and attention masks\n",
    "inputs = tokenizer(prompts, return_tensors='pt', padding=True, truncation=True)\n",
    "\n",
    "# Ensure pad_token_id is set\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Generate text with proper handling of padding tokens\n",
    "outputs = model.generate(\n",
    "    input_ids=inputs['input_ids'],\n",
    "    attention_mask=inputs['attention_mask'],\n",
    "    max_length=50,\n",
    "    num_return_sequences=1,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "# Decode generated texts\n",
    "generated_texts = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
    "\n",
    "# Display results\n",
    "for idx, text in enumerate(generated_texts):\n",
    "    print(f\"Prompt {idx+1}: {prompts[idx]}\")\n",
    "    print(f\"Generated Text: {text}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bbc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
